{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Segmentare_FCN_VGG.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"lsRnP0DRMZT-","colab_type":"code","colab":{}},"source":["#Then you upload the files you already download to your drive\n","#Connect to your google drive\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive/')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"815ed1-EMdbb","colab_type":"code","colab":{}},"source":["#RUN\n","%pip install lycon\n","import torch \n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision.transforms.functional as TF\n","import lycon\n","import torchvision.datasets as dsets\n","from skimage import transform\n","import torchvision.transforms as transforms\n","import torchvision\n","from torch.autograd import Variable\n","import pandas as pd;\n","import numpy as np;\n","from torch.utils.data import Dataset, DataLoader\n","#from vis_utils import *\n","import random;\n","import math;\n","import matplotlib.pyplot as plt\n","import scipy.io as sio\n","import pandas as pd\n","from torchvision import models\n","from torch.optim import lr_scheduler\n","#from tensorboard_logger import configure, log_value\n","import time"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"1-dPJbyusr6s","colab":{}},"source":["#Read the images and ground_truth from drive\n","\n","Fold_1 = sio.loadmat('/content/gdrive/My Drive/Licenta/Fold_1.mat')\n","Fold_2 = sio.loadmat('/content/gdrive/My Drive/Licenta/Fold_2.mat')\n","Fold_3 = sio.loadmat('/content/gdrive/My Drive/Licenta/Fold_3.mat')\n","Fold_4 = sio.loadmat('/content/gdrive/My Drive/Licenta/Fold_4.mat')\n","Fold_5 = sio.loadmat('/content/gdrive/My Drive/Licenta/Fold_5.mat')\n","\n","fold_1 = Fold_1['Fold_1']\n","fold_2 = Fold_2['Fold_2']\n","fold_3 = Fold_3['Fold_3']\n","fold_4 = Fold_4['Fold_4']\n","fold_5 = Fold_5['Fold_5']\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"62Fh7pQgsryG","colab":{}},"source":["#RUN\n","def get_5_fold(index_4, index_1):\n","    x = [None]*5\n","    y = [None]*5\n","    t = [None]*5\n","    x[0] = fold_1[:,0]\n","    x[1] = fold_2[:,0]\n","    x[2] = fold_3[:,0]\n","    x[3] = fold_4[:,0]\n","    x[4] = fold_5[:,0]\n","\n","    y[0] = fold_1[:,1:(512*512)+ 1]\n","    y[1] = fold_2[:,1:(512*512)+ 1]\n","    y[2] = fold_3[:,1:(512*512)+ 1]\n","    y[3] = fold_4[:,1:(512*512)+ 1]\n","    y[4] = fold_5[:,1:(512*512)+ 1]\n","\n","    t[0] = fold_1[:,(512*512)+ 1:2*(512*512)+ 1]\n","    t[1] = fold_2[:,(512*512)+ 1:2*(512*512)+ 1]\n","    t[2] = fold_3[:,(512*512)+ 1:2*(512*512)+ 1]\n","    t[3] = fold_4[:,(512*512)+ 1:2*(512*512)+ 1]\n","    t[4] = fold_5[:,(512*512)+ 1:2*(512*512)+ 1]\n","    \n","    labels = np.concatenate([x[index_4[0]], x[index_4[1]], x[index_4[2]], x[index_4[3]], x[index_1[0]]], 0)\n","    images = np.concatenate([y[index_4[0]], y[index_4[1]], y[index_4[2]], y[index_4[3]], y[index_1[0]]], 0)\n","    masks = np.concatenate([t[index_4[0]], t[index_4[1]], t[index_4[2]], t[index_4[3]], t[index_1[0]]], 0)\n","\n","    \n","    fold_first = x[index_4[0]].shape[0] + x[index_4[1]].shape[0]+  x[index_4[2]].shape[0] + x[index_4[3]].shape[0]\n","    fold_last = x[index_1[0]].shape[0]\n","    del x,y,t\n","    return labels, images, masks, fold_first, fold_last"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7nJaLrmiA2uA","colab_type":"code","outputId":"73676160-64d8-4041-fb01-b50b8b7d7f80","executionInfo":{"status":"ok","timestamp":1561119243666,"user_tz":-180,"elapsed":47528,"user":{"displayName":"Jitca David","photoUrl":"https://lh6.googleusercontent.com/-ULYocHV3lKk/AAAAAAAAAAI/AAAAAAAACF4/Dqy1oAJaMas/s64/photo.jpg","userId":"04674917074172111706"}},"colab":{"base_uri":"https://localhost:8080/","height":72}},"source":["#RUN\n","\n","# Realize 5-fold cross-validation\n","from itertools import combinations \n","  \n","# Get all combinations of [1, 2, 3, 4, 5] \n","# and length 4\n","comb = combinations([0, 1, 2, 3, 4], 4) \n","comb = list(comb)\n","#Choose a combination from 0..4\n","ind = 0\n","res = [ele  for ele in range(5) if ele   not in comb[ind]] \n","\n","#Print first 4-fold indexex for train, and 1-fold index for test\n","print('Indexes for train: ', comb[ind])\n","print('Index for test', res)\n","\n","labels, images, masks, first_fold_nr, last_fold_nr = get_5_fold(comb[ind], res)\n","print(masks.shape, images.shape, first_fold_nr, last_fold_nr)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Indexes for train:  (0, 1, 2, 3)\n","Index for test [4]\n","(3049, 262144) (3049, 262144) 2406 643\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8J4HFX1ERE_p","colab_type":"code","colab":{}},"source":["#RUN\n","#free RAM, because session will crash\n","%reset_selective -f Fold_1\n","%reset_selective -f Fold_2\n","%reset_selective -f Fold_3\n","%reset_selective -f Fold_4\n","%reset_selective -f Fold_5\n","\n","%reset_selective -f fold_1\n","%reset_selective -f fold_2\n","%reset_selective -f fold_3\n","%reset_selective -f fold_4\n","%reset_selective -f fold_5\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sDERDu1MIv6h","colab_type":"code","outputId":"96c35b2e-5dc5-47a0-ad85-03f5465ba0f2","executionInfo":{"status":"ok","timestamp":1561119291760,"user_tz":-180,"elapsed":473,"user":{"displayName":"Jitca David","photoUrl":"https://lh6.googleusercontent.com/-ULYocHV3lKk/AAAAAAAAAAI/AAAAAAAACF4/Dqy1oAJaMas/s64/photo.jpg","userId":"04674917074172111706"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["#RUN\n","# Choose l1 and l2 depending on batch_size, and 5_fold generation\n","# batch should divide de size\n","# Here I hard coded the values for train and test, depending on folders that I have selected for train and test\n","l = [[2400, 630], [2430, 600], [2460, 570], [2370, 630], [2490, 540]]\n","\n","size_train_loader = l[ind][0]\n","size_test_loader = l[ind][1]\n","\n","labels = labels\n","images = images.reshape(3049, -1, 512)\n","masks = masks.reshape(3049, -1, 512)\n","\n","print(labels.shape, images.shape, masks.shape)\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(3049,) (3049, 512, 512) (3049, 512, 512)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UhxlfOfciEyT","colab_type":"code","colab":{}},"source":["# save to disk\n","PATH = \"/content/gdrive/My Drive/Date_Medicale\"\n","num_imgs = images.shape[0]\n","#for i in range(num_imgs):\n","    if i%200 == 0:\n","        print(i, num_imgs)\n","\n","    shape = images[i].shape\n","    img = (images[i]/images[i].max()*255).astype(np.uint8)\n","    gt = (masks[i]*255).astype(np.uint8)\n","    img = np.repeat(img[:, :, None], 3, axis=2)\n","    \n","    img_path = \"%s/Images/%04i.png\" % (PATH, i)\n","    gt_path = \"%s/GTs/%04i.png\" % (PATH, i)\n","    lycon.save(img_path, img)\n","    lycon.save(gt_path, gt)\n","#     fig, (ax0, ax1, ax2, ax3) = plt.subplots(nrows=1, ncols=4, figsize=(16, 7))\n","#     show_img(train_x[i], ax=ax0)\n","#     show_img(img, ax=ax1)\n","#     show_img(train_y[i], ax=ax2)\n","#     show_img(gt, ax=ax3)\n","#     plt.pause(0.0001)\n","    \n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UvSBorWSpekw","colab_type":"text"},"source":["## Data"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-06-03T09:12:16.257393Z","start_time":"2019-06-03T09:12:16.215348Z"},"id":"ieENCd_hpeky","colab_type":"code","colab":{}},"source":["PATH = \"/content/gdrive/My Drive/Date_Medicale/\"\n","load_name = '/data/DAVIS-2016/VladSegmDataset/models/un_model_128x128_v2'\n","save_name = '/data/DAVIS-2016/date_medicale/train/rmn_128x128_v5'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cnsTeSEMVmXc","colab_type":"code","colab":{}},"source":["import glob\n","x_names = np.sort(np.array(glob.glob(PATH + \"Images/*.png\")))\n","y_names = np.sort(np.array(glob.glob(PATH + \"GTs/*.png\")))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-06-03T09:12:16.329370Z","start_time":"2019-06-03T09:12:16.260387Z"},"id":"bN-29emdpek2","colab_type":"code","outputId":"2a182311-2af1-4db5-ef47-d14f61eea6d1","executionInfo":{"status":"ok","timestamp":1561119331855,"user_tz":-180,"elapsed":33421,"user":{"displayName":"Jitca David","photoUrl":"https://lh6.googleusercontent.com/-ULYocHV3lKk/AAAAAAAAAAI/AAAAAAAACF4/Dqy1oAJaMas/s64/photo.jpg","userId":"04674917074172111706"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["x_names = np.sort(np.array(glob.glob(PATH + \"Images/*.png\")))\n","y_names = np.sort(np.array(glob.glob(PATH + \"GTs/*.png\")))\n","\n","train_x = x_names[0: size_train_loader]\n","train_y = y_names[0: size_train_loader]\n","\n","val_x = x_names[first_fold_nr: first_fold_nr + size_test_loader]\n","val_y = y_names[first_fold_nr: first_fold_nr + size_test_loader]\n","\n","print(train_x.shape, train_y.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(2400,) (2400,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-06-03T09:12:16.360006Z","start_time":"2019-06-03T09:12:16.331581Z"},"id":"_pGYH4Hypek8","colab_type":"code","colab":{}},"source":["#Dataloader\n","\n","class SegmentationDataset(Dataset):\n","    def __init__(self, x_paths, y_paths, tr=None):\n","        self.x_paths = x_paths \n","        self.y_paths = y_paths\n","        self.tr = tr\n","        self.transform = torchvision.transforms.ToTensor()\n","        \n","    def transformm(self, image, mask):\n","\n","        # Random horizontal flipping\n","        image = transforms.ToPILImage()(image)\n","        mask = transforms.ToPILImage()(mask)\n","        if random.random() > 0.5:\n","            image = TF.hflip(image)\n","            mask = TF.hflip(mask)\n","\n","        # Random vertical flipping\n","        if random.random() > 0.5:\n","            image = TF.vflip(image)\n","            mask = TF.vflip(mask)\n","            \n","        if random.random() > 0.5:\n","            rotation = random.randint(15, 45)\n","            image = TF.rotate(image, rotation)\n","            mask = TF.rotate(mask, rotation)\n","            \n","        return image, mask\n","    def __getitem__(self, index):\n","        SIZE = 128\n","        image = lycon.load(self.x_paths[index])\n","        image = lycon.resize(image, width=SIZE, height=SIZE)\n","        \n","        mask = lycon.load(self.y_paths[index])\n","        mask = lycon.resize(mask, width=SIZE, height=SIZE)\n","        mask = mask[:, :, 0]\n","        \n","        if self.tr == 1:\n","          image, mask = self.transformm(image, mask)\n","        # Transform to tensor\n","        image = self.transform(image)\n","        mask = self.transform(mask)\n","        return image, mask\n","\n","    def __len__(self):\n","        return len(self.x_paths)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"J4wkTVWZep3k","colab_type":"code","colab":{}},"source":["def show_img(im, figsize=None, ax=None, alpha=None):\n","    if not ax: fig,ax = plt.subplots(figsize=figsize)\n","    ax.imshow(im, alpha=alpha)\n","#     ax.set_axis_off()\n","    ax.grid()\n","    return ax"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pl8srUZ2pvKo","colab_type":"code","colab":{}},"source":["#Arhitecture of segmentation model, VGG encoder\n","\n","import torch, numpy as np, os.path as osp, torch.nn as nn\n","\n","def cached_download(url, path, md5=None, quiet=False, postprocess=None):\n","\n","    def check_md5(path, md5):\n","        print(('[{:s}] Checking md5 ({:s})').format(path, md5))\n","        return md5sum(path) == md5\n","\n","    if osp.exists(path):\n","        if not md5:\n","            print(('[{:s}] File exists ({:s})').format(path, md5sum(path)))\n","        if osp.exists(path):\n","            if md5:\n","                if check_md5(path, md5):\n","                    pass\n","                dirpath = osp.dirname(path)\n","                if not osp.exists(dirpath):\n","                    os.makedirs(dirpath)\n","                gdown.download(url, path, quiet=quiet)\n","        if postprocess is not None:\n","            postprocess(path)\n","        return path\n","\n","\n","def get_upsampling_weight(in_channels, out_channels, kernel_size):\n","    \"\"\"Make a 2D bilinear kernel suitable for upsampling\"\"\"\n","    factor = (kernel_size + 1) // 2\n","    if kernel_size % 2 == 1:\n","        center = factor - 1\n","    else:\n","        center = factor - 0.5\n","    og = np.ogrid[:kernel_size, :kernel_size]\n","    filt = (1 - abs(og[0] - center) / factor) * (1 - abs(og[1] - center) / factor)\n","    weight = np.zeros((in_channels, out_channels, kernel_size, kernel_size), dtype=(np.float64))\n","    weight[range(in_channels), range(out_channels), :, :] = filt\n","    return torch.from_numpy(weight).float()\n","\n","\n","class FCN8s(nn.Module):\n","    pretrained_model = osp.expanduser('~/data/models/pytorch/fcn8s_from_caffe.pth')\n","\n","    @classmethod\n","    def download(cls):\n","        return cached_download(url='http://drive.google.com/uc?id=0B9P1L--7Wd2vT0FtdThWREhjNkU',\n","          path=(cls.pretrained_model),\n","          md5='dbd9bbb3829a3184913bccc74373afbb')\n","\n","    def __init__(self, n_class, vgg):\n","        super().__init__()\n","        self.conv1_1 = nn.Conv2d(3, 64, 3, padding=100)\n","        self.relu1_1 = nn.ReLU(inplace=True)\n","        self.conv1_2 = nn.Conv2d(64, 64, 3, padding=1)\n","        self.relu1_2 = nn.ReLU(inplace=True)\n","        self.pool1 = nn.MaxPool2d(2, stride=2, ceil_mode=True)\n","        self.conv2_1 = nn.Conv2d(64, 128, 3, padding=1)\n","        self.relu2_1 = nn.ReLU(inplace=True)\n","        self.conv2_2 = nn.Conv2d(128, 128, 3, padding=1)\n","        self.relu2_2 = nn.ReLU(inplace=True)\n","        self.pool2 = nn.MaxPool2d(2, stride=2, ceil_mode=True)\n","        self.conv3_1 = nn.Conv2d(128, 256, 3, padding=1)\n","        self.relu3_1 = nn.ReLU(inplace=True)\n","        self.conv3_2 = nn.Conv2d(256, 256, 3, padding=1)\n","        self.relu3_2 = nn.ReLU(inplace=True)\n","        self.conv3_3 = nn.Conv2d(256, 256, 3, padding=1)\n","        self.relu3_3 = nn.ReLU(inplace=True)\n","        self.pool3 = nn.MaxPool2d(2, stride=2, ceil_mode=True)\n","        self.conv4_1 = nn.Conv2d(256, 512, 3, padding=1)\n","        self.relu4_1 = nn.ReLU(inplace=True)\n","        self.conv4_2 = nn.Conv2d(512, 512, 3, padding=1)\n","        self.relu4_2 = nn.ReLU(inplace=True)\n","        self.conv4_3 = nn.Conv2d(512, 512, 3, padding=1)\n","        self.relu4_3 = nn.ReLU(inplace=True)\n","        self.pool4 = nn.MaxPool2d(2, stride=2, ceil_mode=True)\n","        self.conv5_1 = nn.Conv2d(512, 512, 3, padding=1)\n","        self.relu5_1 = nn.ReLU(inplace=True)\n","        self.conv5_2 = nn.Conv2d(512, 512, 3, padding=1)\n","        self.relu5_2 = nn.ReLU(inplace=True)\n","        self.conv5_3 = nn.Conv2d(512, 512, 3, padding=1)\n","        self.relu5_3 = nn.ReLU(inplace=True)\n","        self.pool5 = nn.MaxPool2d(2, stride=2, ceil_mode=True)\n","        self.fc6 = nn.Conv2d(512, 4096, 7)\n","        self.relu6 = nn.ReLU(inplace=True)\n","        self.drop6 = nn.Dropout2d()\n","        self.fc7 = nn.Conv2d(4096, 4096, 1)\n","        self.relu7 = nn.ReLU(inplace=True)\n","        self.drop7 = nn.Dropout2d()\n","        self.score_fr = nn.Conv2d(4096, n_class, 1)\n","        self.score_pool3 = nn.Conv2d(256, n_class, 1)\n","        self.score_pool4 = nn.Conv2d(512, n_class, 1)\n","        self.upscore2 = nn.ConvTranspose2d(n_class,\n","          n_class, 4, stride=2, bias=False)\n","        self.upscore8 = nn.ConvTranspose2d(n_class,\n","          n_class, 16, stride=8, bias=False)\n","        self.upscore_pool4 = nn.ConvTranspose2d(n_class,\n","          n_class, 4, stride=2, bias=False)\n","        self._initialize_weights()\n","        self.copy_params_from_vgg16(vgg)\n","\n","    def _initialize_weights(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                m.weight.data.zero_()\n","                if m.bias is not None:\n","                    m.bias.data.zero_()\n","            if isinstance(m, nn.ConvTranspose2d):\n","                if not m.kernel_size[0] == m.kernel_size[1]:\n","                    raise AssertionError\n","                initial_weight = get_upsampling_weight(m.in_channels, m.out_channels, m.kernel_size[0])\n","                m.weight.data.copy_(initial_weight)\n","\n","    def forward(self, x):\n","        h = x\n","        h = self.relu1_1(self.conv1_1(h))\n","        h = self.relu1_2(self.conv1_2(h))\n","        h = self.pool1(h)\n","        h = self.relu2_1(self.conv2_1(h))\n","        h = self.relu2_2(self.conv2_2(h))\n","        h = self.pool2(h)\n","        h = self.relu3_1(self.conv3_1(h))\n","        h = self.relu3_2(self.conv3_2(h))\n","        h = self.relu3_3(self.conv3_3(h))\n","        h = self.pool3(h)\n","        pool3 = h\n","        h = self.relu4_1(self.conv4_1(h))\n","        h = self.relu4_2(self.conv4_2(h))\n","        h = self.relu4_3(self.conv4_3(h))\n","        h = self.pool4(h)\n","        pool4 = h\n","        h = self.relu5_1(self.conv5_1(h))\n","        h = self.relu5_2(self.conv5_2(h))\n","        h = self.relu5_3(self.conv5_3(h))\n","        h = self.pool5(h)\n","        h = self.relu6(self.fc6(h))\n","        h = self.drop6(h)\n","        h = self.relu7(self.fc7(h))\n","        h = self.drop7(h)\n","        h = self.score_fr(h)\n","        h = self.upscore2(h)\n","        upscore2 = h\n","        h = self.score_pool4(pool4)\n","        h = h[:, :, 5:5 + upscore2.size()[2], 5:5 + upscore2.size()[3]]\n","        score_pool4c = h\n","        h = upscore2 + score_pool4c\n","        h = self.upscore_pool4(h)\n","        upscore_pool4 = h\n","        h = self.score_pool3(pool3)\n","        h = h[:, :,\n","         9:9 + upscore_pool4.size()[2],\n","         9:9 + upscore_pool4.size()[3]]\n","        score_pool3c = h\n","        h = upscore_pool4 + score_pool3c\n","        h = self.upscore8(h)\n","        h = h[:, :, 31:31 + x.size()[2], 31:31 + x.size()[3]].contiguous()\n","        return h\n","\n","    def copy_params_from_vgg16(self, vgg16):\n","        features = [\n","         self.conv1_1, self.relu1_1,\n","         self.conv1_2, self.relu1_2,\n","         self.pool1,\n","         self.conv2_1, self.relu2_1,\n","         self.conv2_2, self.relu2_2,\n","         self.pool2,\n","         self.conv3_1, self.relu3_1,\n","         self.conv3_2, self.relu3_2,\n","         self.conv3_3, self.relu3_3,\n","         self.pool3,\n","         self.conv4_1, self.relu4_1,\n","         self.conv4_2, self.relu4_2,\n","         self.conv4_3, self.relu4_3,\n","         self.pool4,\n","         self.conv5_1, self.relu5_1,\n","         self.conv5_2, self.relu5_2,\n","         self.conv5_3, self.relu5_3,\n","         self.pool5]\n","        for l1, l2 in zip(vgg16.features, features):\n","            if isinstance(l1, nn.Conv2d) and isinstance(l2, nn.Conv2d):\n","                if not l1.weight.size() == l2.weight.size():\n","                    raise AssertionError\n","                if not l1.bias.size() == l2.bias.size():\n","                    raise AssertionError\n","                l2.weight.data.copy_(l1.weight.data)\n","                l2.bias.data.copy_(l1.bias.data)\n","                #l2.weight.requires_grad = False\n","                #l2.bias.requires_grad = False\n","\n","        for i, name in zip([0, 3], ['fc6', 'fc7']):\n","            l1 = vgg16.classifier[i]\n","            l2 = getattr(self, name)\n","            l2.weight.data.copy_(l1.weight.data.view(l2.weight.size()))\n","            l2.bias.data.copy_(l1.bias.data.view(l2.bias.size()))\n","# okay decompiling fcn8s.cpython-36.pyc\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-06-03T09:12:16.388368Z","start_time":"2019-06-03T09:12:16.362386Z"},"id":"UjwxOfoxpelA","colab_type":"code","colab":{}},"source":["trainset = SegmentationDataset(train_x, train_y, 1)\n","valset = SegmentationDataset(val_x, val_y)\n","train_loader = torch.utils.data.DataLoader(trainset, batch_size=30, shuffle=True, num_workers=4)\n","val_loader = torch.utils.data.DataLoader(valset, batch_size=30, shuffle=False, num_workers=4)\n","\n","test_loader = torch.utils.data.DataLoader(valset, batch_size=1, shuffle=False, num_workers=4)\n","trtest_loader =  torch.utils.data.DataLoader(trainset, batch_size=1, shuffle=False, num_workers=4)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SShpNQ_QaGGH","colab_type":"code","colab":{}},"source":["#RUN to test if data is normalized, and mask is correlated with image\n","#Printez sa vad daca s-a normalizat\n","#Print values from an image to see if values are normalized\n","\n","t, l = trainset.__getitem__(2399)\n","print(l.shape)\n","print(t.shape)\n","#print(t[0][75])\n","print(trainset.__len__())\n","plt.figure(1)\n","plt.subplot(211)\n","plt.imshow(t.permute(1, 2, 0))\n","\n","plt.subplot(212)\n","plt.imshow(l[0])\n","plt.show()\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YM3cz6XlEArx","colab_type":"code","colab":{}},"source":["#Function to calculate dice between 2 images\n","def T_dice_coefficient(predicted, target):\n","        \"\"\"Calculates the Sørensen–Dice Coefficient for a\n","        single sample.\n","        Parameters:\n","            predicted(numpy.ndarray): Predicted single output of the network.\n","                                    Shape - (Channel,Height,Width)\n","            target(numpy.ndarray): Actual required single output for the network\n","                                    Shape - (Channel,Height,Width)\n","        Returns:\n","            coefficient(float): Dice coefficient for the input sample.\n","                                        1 represents high similarity and\n","                                        0 represents low similarity.\n","        \"\"\"\n","        smooth = 1\n","       # predicted = predicted.cpu().numpy()\n","        #target = target.cpu().numpy()\n","        product = np.multiply(predicted, target)\n","        intersection = np.sum(product)\n","        coefficient = (2*intersection + smooth) / \\\n","            (np.sum(predicted) + np.sum(target) + smooth)\n","        return coefficient\n","\n","#Accuracy metric function, dice score\n","def MeanDice(model, loader, len):\n","    total_score = 0\n","    model.eval()\n","    for i, (img, mask_t) in enumerate(loader):\n","        \n","        sizee = img.shape[0]\n","        if sizee != 1:\n","           raise Exception(\"Set batch size to 1 for testing purpose\")\n","        else:\n","          \n","          data = img.to(device).float()\n","          with torch.no_grad():\n","             mask_pred = model(data)\n","         \n","          mask = mask_t[0][0]\n","          mask_pred = mask_pred[0][0]\n","\n","          output = (mask_pred > 0.5)\n","          output = output.cpu().numpy()\n","          output = np.resize(output, (size, size))\n","          mask = np.resize(mask,(size, size))\n","          iou_score = T_dice_coefficient(output, mask)\n","          total_score += iou_score\n","    print(total_score)\n","    model.train()\n","    return total_score/len"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"LYAVHpLr-hNl","colab":{}},"source":["#Accuracy metric function, IOU\n","def MeanIOU(model, loader, len, treshold = 0.5):\n","    total_score = 0\n","    for i, (img, mask_t) in enumerate(loader):\n","       \n","        sizee = img.shape[0]\n","        \n","        if sizee != 1:\n","           raise Exception(\"Set batch size to 1 for testing purpose\")\n","        else:  \n","          data = img.to(device).float()\n","          with torch.no_grad():\n","             mask_pred = model(data)\n","         \n","          mask = mask_t[0][0]\n","          mask_pred = mask_pred[0][0]\n","          output = (mask_pred > treshold)\n","          output = output.cpu().numpy()\n","          output = np.resize(output, (size, size))\n","          intersection = np.logical_and(mask, output)\n","          union = np.logical_or(mask, output)\n","          iou_score = np.sum(intersection.cpu().detach().numpy()) / np.sum(union.cpu().detach().numpy())\n","          total_score += iou_score\n","    print(total_score)\n","    return total_score/len"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_M9NZMaFbhJF","colab_type":"code","colab":{}},"source":["#IOU function for binary-segmentation\n","def iou_cross(model, loader, len):\n","    total = 0\n","    to = 0\n","    for i, (img, mask_t) in enumerate(loader):\n","        \n","        data = img.expand(-1, 3, -1, -1)\n","        data = data.to(device).float()\n","        sizee = img.shape[0]\n","        data = img.to(device).float()\n","\n","        if sizee != 1:\n","           raise Exception(\"Set batch size to 1 for testing purpose\")\n","        else:\n","          with torch.no_grad(): \n","            mask_pred = model(data)\n","\n","        mask = mask_t[0]\n","        tt = mask_pred[0]\n","        lbl_pred = tt.max(0)[1].cpu()\n","        intersection = np.logical_and(mask, lbl_pred)\n","        union = np.logical_or(mask, lbl_pred)\n","        iou_score = np.sum(intersection.cpu().detach().numpy()) / np.sum(union.cpu().detach().numpy())\n","        total += iou_score\n","        \n","    return total/len"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"b5LhDRuM0snG","colab_type":"code","colab":{}},"source":["#dice function for binary-segmentation\n","def dice_cross(model, loader, len):\n","    total = 0\n","    for i, (img, mask_t) in enumerate(loader):\n","        \n","        data = img.expand(-1, 3, -1, -1)\n","        data = data.to(device).float()\n","        sizee = img.shape[0]\n","        data = img.to(device).float()\n","\n","        if sizee != 1:\n","           raise Exception(\"Set batch size to 1 for testing purpose\")\n","        else:\n","          with torch.no_grad(): \n","            mask_pred = model(data)\n","\n","        mask = mask_t[0]\n","        tt = mask_pred[0]\n","        lbl_pred = tt.max(0)[1].cpu()  \n","        \n","        iou_score = T_dice_coefficient(lbl_pred, mask)\n","        total += iou_score\n","        \n","    return total/len"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7KmICzLOwXJh","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-06-03T09:12:22.909938Z","start_time":"2019-06-03T09:12:16.680938Z"},"id":"U5oU27q0pelE","colab_type":"code","colab":{}},"source":["#criterion = nn.BCEWithLogitsLoss()\n","n_classes = 2\n","lr = 1e-5\n","batch_size = 30\n","size = 128\n","\n","#Load pretrained model\n","vgg = models.vgg16(pretrained=True)\n","fcn_model = FCN8s(n_classes, vgg).cuda()\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wFyE3U_3yygl","colab_type":"code","colab":{}},"source":["#Set hyperparameters for training\n","epochs = 1\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","lr = 0.000011\n","optimizer = torch.optim.Adam(fcn_model.parameters(), lr=lr)\n","criterion = nn.BCEWithLogitsLoss()\n","#scheduler  = lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.75)\n","scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.85, patience=3, verbose=True)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ocRUwUJ65udQ","colab_type":"code","colab":{}},"source":["torch.cuda.empty_cache()\n","print(optimizer)\n","print(criterion)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lBY98GKD3bhO","colab_type":"code","colab":{}},"source":["#Load model from path in drive\n","\n","path = 'gdrive/My Drive/Licenta/FCN8s_dice/fcn_8s_B_dice.pt'\n","fcn_model.load_state_dict(torch.load(path))\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YGc2ES_Z0cXz","colab_type":"code","colab":{}},"source":["#List of loses for plots\n","\n","train_losses = []\n","test_losses = []\n","IOU = []\n","Dice = []\n","best_loss = 100000\n","\n","#Path save\n","path_save = 'gdrive/My Drive/Licenta/FCN8s_dice/fcn_8s_B_diceee.pt'\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"start_time":"2019-06-03T09:13:08.684Z"},"scrolled":false,"id":"NBX0RFaypelI","colab_type":"code","colab":{}},"source":["#Train function\n","\n","def train(best_loss, epochs):\n","    best = best_loss \n","    for epoch in range(epochs):\n","        \n","        ts = time.time()\n","        fcn_model.train()\n","        loss_per_epoch = 0\n","        mean_loss = []\n","        \n","        for iter_no, (inputs, labels) in enumerate(train_loader):\n","            # train\n","            inputs, labels = inputs.cuda(), labels.cuda()\n","            inputs = inputs.expand(-1, 3, -1, -1)\n","            #labels = labels.long()\n","            #labels = labels.view(-1, size, size)\n","            \n","            optimizer.zero_grad()\n","            outputs = fcn_model(inputs)[:,:1]\n","\n","            loss = criterion(outputs, labels)\n","            loss /= inputs.shape[0]\n","            loss.backward()\n","            optimizer.step()\n","\n","            mean_loss.append(loss.detach().cpu())\n","            \n","            if iter_no%10 == 0:\n","                print (\"epoch {}, iter {}, loss: {}\".format(epoch, iter_no, loss))\n","        print('--------------END EPOCH--------------')\n","        print(\"Finish epoch {}, time elapsed {}\".format(epoch, time.time() - ts))\n","        print(\"Epoch\", epoch, \"mean_loss\", np.mean(mean_loss))\n","\n","        train_losses.append( np.mean(mean_loss))\n","\n","        idx = np.random.randint(len(val_loader))\n","        fcn_model.eval()\n","        val_inputs, val_labels = valset[idx]\n","        print(idx)\n","        val_inputs, val_labels = val_inputs.cuda(), val_labels.cuda()\n","        val_outputs = fcn_model(val_inputs[None])[:, :1]\n","        show_img(val_inputs[0].data.cpu().numpy())\n","        plt.pause(0.00001)\n","        show_img(val_labels[0].data.cpu().numpy())\n","        plt.pause(0.00001)\n","        show_img(val_outputs[0, 0].data.cpu().numpy())\n","        plt.pause(0.00001)\n","        fcn_model.train()\n","\n","        print('--------------Start VALID--------------')\n","\n","        ep_loss = validate(fcn_model)\n","        scheduler.step(ep_loss)   \n","        ac = MeanIOU(fcn_model, test_loader, size_test_loader)\n","        dice = MeanDice(fcn_model, test_loader, size_test_loader)\n","        #ac = iou_cross(fcn_model, test_loader, size_test_loader)\n","        #dice = dice_cross(fcn_model, test_loader, size_test_loader)\n","        \n","        print('Accuracy:', ac)\n","        IOU.append(ac)\n","        Dice.append(dice)\n","\n","        if ep_loss < best:\n","          print(\"Save model:\")\n","          torch.save(fcn_model.state_dict(),path_save)\n","          best = ep_loss\n","    return best\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kk2JL9JGs5hC","colab_type":"code","colab":{}},"source":["#Validation function\n","\n","def validate(model):\n","        \n","        fcn_model.eval()\n","        visualizations = []\n","        mean_loss = []\n","        model.eval()\n","        for iter, (inputs, labels) in enumerate(val_loader):\n","            \n","            optimizer.zero_grad()\n","            inputs = inputs.cuda()\n","            labels = labels.cuda()\n","            #labels = labels.long()\n","            inputs = inputs.expand(-1, 3, -1, -1)\n","            #labels = labels.view(-1, size, size)\n","            \n","            with torch.no_grad():\n","                score = model(inputs)[:,:1]\n","                       \n","            loss = criterion(score, labels)\n","            loss /= inputs.shape[0]\n","            mean_loss.append(loss.detach().cpu())\n","\n","            if np.isnan(loss.detach().cpu()):\n","                raise ValueError('loss is nan while validating')\n","            if iter%10 == 0:\n","                print (\"iter {}, loss: {}\".format( iter, loss))\n","                \n","        print('Loss: ', np.mean(mean_loss))\n","        test_losses.append(np.mean(mean_loss))\n","        print('=================END_VALID===============')\n","        model.train()\n","        return np.mean(mean_loss)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5gjOCkA6ibum","colab_type":"code","colab":{}},"source":["epochs = 5\n","best = train(best_loss, epochs)\n","best_loss = best"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HvdDv41CvN27","colab_type":"code","colab":{}},"source":["#Plot & save plots function\n","from matplotlib import pyplot\n","\n","def plot_loss(loss, label, name, xlabel, ylabel, color='blue'):\n","    pyplot.plot(loss, label=label, color=color)\n","    pyplot.xlabel(xlabel)\n","    pyplot.ylabel(ylabel)\n","    pyplot.legend()\n","    pyplot.savefig(name)\n","def save_plot(loss, label, name, color='blue'):\n","    pyplot.plot(loss, label=label, color=color)\n","    pyplot.legend()\n","    pyplot.savefig(name)\n","nume = 'FCN8s_bce)aug.png'\n","def plot_loss_t_t(loss1, loss2, label1, label2, name, color1='blue', color2='red'):\n","    pyplot.plot(loss1, label=label1, color=color1)\n","    pyplot.plot(loss2, label=label2, color=color2)\n","    pyplot.xlabel(\"Epoca\")\n","    pyplot.ylabel(\"Eroare\")\n","    pyplot.legend()\n","    pyplot.savefig(name)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-ltKI2QX9zq7","colab_type":"code","colab":{}},"source":["\n","plot_loss_t_t(train_losses, test_losses, 'train_loss', 'test_loss', '/content/gdrive/My Drive/Licenta/FCN8s_aug/train_test_loss' + nume,'red', 'blue')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eX3jbgFI1BdT","colab_type":"code","colab":{}},"source":["plot_loss(IOU, 'IOU', '/content/gdrive/My Drive/Licenta/FCN8s_cross/mean_iou_' + nume,  'Epoca', 'Acuratete IOU', 'red')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"unYukfkyG83o","colab_type":"code","colab":{}},"source":["plot_loss(Dice, 'Scor Dice', '/content/gdrive/My Drive/Licenta/FCN8s_cross/dice_' + nume,'Epoca', 'Scor Dice' ,'red')\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nNwrs48jxbkC","colab_type":"code","colab":{}},"source":["#Evaluate 1 image\n","\n","t = fcn_model.eval()\n","ind = 4                #Select the index of image from [0, batch_size - 1]\n","\n","it = iter(val_loader)\n","\n","img1, mask_t = next(it)\n","img = img1\n","print(img1.shape, mask_t.shape, img.shape)\n","\n","\n","img1 = img1.cuda()\n","mask_t = mask_t[ind].view(-1, 128)\n","mask_pred = fcn_model(img1)[:, :1]\n","\n","mask_pred = mask_pred[ind]\n","\n","\n","\n","print(mask_pred.shape)\n","output = (mask_pred > 0.5)\n","output = output.cpu().numpy()\n","output = np.resize(output, (128, 128))\n","\n","\n","t1 = mask_t[mask_t > 0 ].sum()\n","t2 = output[output > 0].sum()\n","print('----------------------------------------------------')\n","print(\"Nr Pixeli din tumoare: \", t1, \"Pixeli prezisi: \", t2)  \n","\n","\n","f, axarr = plt.subplots(2,2)\n","axarr[0,0].imshow(img[ind][0].cpu())\n","axarr[0,1].imshow(mask_t)\n","axarr[1,0].imshow(img1[ind].cpu().permute(1, 2, 0), cmap = 'gray')\n","axarr[1,1].imshow(output)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mpvzlUOtKeGG","colab_type":"code","colab":{}},"source":["#Function to calculate precision and recall for segmentation\n","\n","def pixel_wise_boundary_precision_recall(pred, gt):\n","    \"\"\"Evaluate voxel prediction accuracy against a ground truth.\n","    Parameters\n","    ----------\n","    pred : np.ndarray of int or bool, arbitrary shape\n","        The voxel-wise discrete prediction. 1 for boundary, 0 for non-boundary.\n","    gt : np.ndarray of int or bool, same shape as `pred`\n","        The ground truth boundary voxels. 1 for boundary, 0 for non-boundary.\n","    Returns\n","    -------\n","    pr : float\n","    rec : float\n","        The precision and recall values associated with the prediction.\n","    Notes\n","    -----\n","    Precision is defined as \"True Positives / Total Positive Calls\", and\n","    Recall is defined as \"True Positives / Total Positives in Ground Truth\".\n","    This function only calculates this value for discretized predictions,\n","    i.e. it does not work with continuous prediction confidence values.\n","    \"\"\"\n","    tp = float((gt * pred).sum())\n","    fp = (pred * (1-gt)).sum()\n","    fn = (gt * (1-pred)).sum()\n","    if tp == 0:\n","      return 0, 0\n","    return tp/(tp+fp), tp/(tp+fn)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yp_WysDkMlSz","colab_type":"code","colab":{}},"source":["from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import precision_score, \\\n","    recall_score, confusion_matrix, classification_report, \\\n","    accuracy_score, f1_score, precision_recall_fscore_support\n","\n","pred = torch.empty(1, 128, 128)\n","lab = torch.empty(1, 128, 128)\n","\n","for i, (images, labels) in enumerate(test_loader):\n","          images = images.to(device).float()         # Convert torch tensor to Variable: change image from a vector of size 784 to a matrix of 28 x 28\n","          labels = labels.to(device).long()\n","         \n","          images = images.expand(-1, 3, -1, -1) #Because first layer of cnns have 3 channels for input and my image have 1 channel, i expand the input to 3 channels\n","          images = images.to(device).float()\n","         \n","          outputs = fcn_model(images)\n","           \n","          mask_pred = outputs[0][0]\n","          output = (mask_pred > 0.5)\n","          output = output.reshape(-1, 128, 128)\n","          labels = labels.view(-1, 128, 128)\n","          \n","          pred = torch.cat((pred, output.cpu().float()), 0)\n","          lab = torch.cat((lab, labels.cpu().float()), 0)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kIFmaNiiLqa5","colab_type":"code","colab":{}},"source":["lab = lab[1:631]\n","pred = pred[1:631]\n","pr = 0\n","re = 0\n","for i in range(630):\n","  t1, t2 = pixel_wise_boundary_precision_recall(pred[i], lab[i])\n","  pr += t1\n","  re += t2\n","  \n","print(pr/630, re/630)\n","\n"],"execution_count":0,"outputs":[]}]}